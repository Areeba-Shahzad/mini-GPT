{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PA4 Part 2. Transformers for Language Modeling [80 Marks]\n",
    "\n",
    "<center>\n",
    "    <img src=\"./assets/transformers.png\">\n",
    "</center>\n",
    "\n",
    "### Introduction\n",
    "\n",
    "In this notebook, you will be implementing a Transformer, `SastaGPT`, from scratch. This will be taken from the paper [Attention Is All You Need](https://arxiv.org/abs/1706.03762) by Vaswani et al. (2017). You will then train this Transformer on a language modeling task on a dataset of your choosing.\n",
    "\n",
    "After this notebook, you should be able to:\n",
    "\n",
    "- Understand how Transformers, and all its components, are implemented in code.\n",
    "\n",
    "- Train a Transformer on a language modeling task.\n",
    "\n",
    "- Brag on Twitter about how you created a Transformer from scratch.\n",
    "\n",
    "### Instructions\n",
    "\n",
    "- Follow along with the notebook, filling out the necessary code where instructed.\n",
    "\n",
    "- <span style=\"color: red;\">Read the Submission Instructions and Plagiarism Policy in the attached PDF.</span>\n",
    "\n",
    "- <span style=\"color: red;\">Make sure to run all cells for credit.</span>\n",
    "\n",
    "- <span style=\"color: red;\">Do not remove any pre-written code.</span> We will be using the `print` statements to grade your assignment.\n",
    "\n",
    "- <span style=\"color: red;\">You must attempt all parts.</span> Do not assume that because something is for 0 marks, you can leave it - it will definitely be used in later parts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tiktoken in c:\\users\\areeba shahzad\\anaconda3\\lib\\site-packages (0.5.2)Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied: regex>=2022.1.18 in c:\\users\\areeba shahzad\\anaconda3\\lib\\site-packages (from tiktoken) (2022.7.9)\n",
      "Requirement already satisfied: requests>=2.26.0 in c:\\users\\areeba shahzad\\anaconda3\\lib\\site-packages (from tiktoken) (2.31.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\areeba shahzad\\anaconda3\\lib\\site-packages (from requests>=2.26.0->tiktoken) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\areeba shahzad\\anaconda3\\lib\\site-packages (from requests>=2.26.0->tiktoken) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\areeba shahzad\\anaconda3\\lib\\site-packages (from requests>=2.26.0->tiktoken) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\areeba shahzad\\anaconda3\\lib\\site-packages (from requests>=2.26.0->tiktoken) (2023.7.22)\n"
     ]
    }
   ],
   "source": [
    "pip install tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import tiktoken"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a Dataset [20 Marks]\n",
    "\n",
    "In this section, you will be creating a dataset for training your Transformer. You are allowed to use any dataset you want, as long as it is a text dataset.\n",
    "\n",
    "Some fun options include:\n",
    "\n",
    "- A collection of movie scripts from your favorite director. Look through [IMSDB](https://www.imsdb.com/) for some options.\n",
    "\n",
    "- A novel from your favorite author. Look through [Project Gutenberg](https://www.gutenberg.org/) for some options.\n",
    "\n",
    "- A collection of poems from your favorite poet. Look through [Poetry Foundation](https://www.poetryfoundation.org/) for some options.\n",
    "\n",
    "- Anything else from [Kaggle](https://www.kaggle.com/datasets) or [HuggingFace](https://huggingface.co/datasets).\n",
    "\n",
    "You are not allowed to be boring and use the same dataset as something previously seen in the course.\n",
    "\n",
    "This section involves doing the following:\n",
    "\n",
    "1. **Finding/Downloading/Creating your dataset**. Add in a few comments about what it is and why you chose it. (10 Marks)\n",
    "\n",
    "2. **Preprocessing your dataset**. This involves tokenizing your dataset, and creating a vocabulary. Previously you used character-level tokenization, now you will use **subword-level tokenization** (you can read about different tokenization strategies [here](https://huggingface.co/learn/nlp-course/chapter2/4?fw=pt)). This can be very easily handled with the `tiktoken` library (read up [here](https://github.com/openai/tiktoken)), which is also what the GPT family uses :p (10 Marks)\n",
    "\n",
    "**Note:** Do not use character-level tokenization (what you used in the previous assignment)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 8]) torch.Size([32, 8])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "BLOCK_SIZE = 8\n",
    "\n",
    "encoder = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "# Read in your dataset using np.loadtxt\n",
    "# i used the script of the movie, kungfu panda as my datset as it is lengthy enough to properly train my models and\n",
    "# parameters, morover since its an animated movie it won't always have the most logically sounding sentence which\n",
    "# i wanted to see if my model would be able to grasp that. \n",
    "file_path = r'C:\\Users\\Areeba Shahzad\\Desktop\\ML\\PA4\\kungfu_panda.txt'\n",
    "with open(file_path, 'r', encoding='utf-8') as file:\n",
    "    text = file.read()\n",
    "\n",
    "text_array = np.array(list(text))  # Assuming each character is a separate element in the array\n",
    "\n",
    "# Complete the get_batch function\n",
    "def get_batch():\n",
    "    '''\n",
    "    Returns a batch (x, y) from the dataset\n",
    "    '''\n",
    "    batch_size = BATCH_SIZE\n",
    "    block_size = BLOCK_SIZE\n",
    "    dataset = text_array\n",
    "    if len(dataset) <= block_size:\n",
    "        raise ValueError(\"Dataset has insufficient data for the specified block size.\")\n",
    "    \n",
    "    # Convert to string\n",
    "    text_str = ''.join(dataset)\n",
    "    \n",
    "    # Get random integers for indexing into the dataset to create a batch\n",
    "    indices = np.random.randint(0, len(dataset) - block_size - 1, batch_size)\n",
    "    \n",
    "    # Get the x and y (input and target) batches after encoding and indexing\n",
    "    x = []\n",
    "    y = []\n",
    "    \n",
    "    for i in indices:\n",
    "        x_block = [ord(char) for char in text_str[i:i + block_size]]\n",
    "        y_block = [ord(char) for char in text_str[i + 1:i + 1 + block_size]]\n",
    "        x.append(x_block)\n",
    "        y.append(y_block)\n",
    "    \n",
    "    # Padding\n",
    "    max_length = max(max(len(seq) for seq in x), max(len(seq) for seq in y))\n",
    "    x = [seq + [0] * (max_length - len(seq)) for seq in x]\n",
    "    y = [seq + [0] * (max_length - len(seq)) for seq in y]\n",
    "    \n",
    "    # Make sure these are int64 (long) tensors\n",
    "    x = torch.tensor(x, dtype=torch.int64)\n",
    "    y = torch.tensor(y, dtype=torch.int64)\n",
    "    \n",
    "    return x, y\n",
    "\n",
    "xb, yb = get_batch()\n",
    "print(xb.shape, yb.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the Transformer\n",
    "\n",
    "### Defining our Hyperparameters [0 Marks]\n",
    "\n",
    "We will define our hyperparameters here. You can play around with these later to see how they affect your model.\n",
    "\n",
    "Since there are lots of hyperparameters to keep track of, it is easier for us to wrap all of them inside a `dataclass`. This can be passed around very easily to different functions, and makes it easier to keep track of all relevant values.\n",
    "\n",
    "To quickly describe some of the hyperparameters:\n",
    "\n",
    "1. `block_size`: This is the length of the sequence that we will be feeding into our model. This has also been called the *context length* and *window size* in other places.\n",
    "\n",
    "2. `emb_dim`: This is the dimensionality of the embeddings we will have inside the model. This has implications on the outputs of tensors throughout the entire model.\n",
    "\n",
    "3. `head_size`: When dealing with Multi-Head Attention, we will be *splitting* our embeddings into multiple heads. This is the size of each of those heads. For example, if we had an embedding of size 512, and we wanted 8 heads, then each head would have a size of 64. Down the line, we'd be concatenating these heads together, so the final output would be of size 512 again, as you should recall from the lectures.\n",
    "\n",
    "4. `num_heads`: This is the number of attention heads we will have in our Multi-Head Attention layer.\n",
    "\n",
    "5. `num_layers`: This is the number of layers we will have in our Transformer (this includes the MHSA, the Feedforward module, and the Layer Normalizations).\n",
    "\n",
    "6. `vocab_size`: This is the size of our vocabulary. This is the number of unique tokens we have in our vocabulary. If you were to use character-level tokenization, this would be the number of unique characters in your dataset. Since you should be using subword-level tokenization, this will be the number of unique subwords in your dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Config:\n",
    "    block_size: int = BLOCK_SIZE\n",
    "    emb_dim: int = 256\n",
    "    head_size: int = 32\n",
    "    num_heads: int = 8\n",
    "    num_layers: int = 2\n",
    "    vocab_size: int = encoder.n_vocab # vocab size of the tokenizer\n",
    "\n",
    "# We like to have emb_dim == head_size * num_heads\n",
    "config = Config()\n",
    "assert config.emb_dim == config.head_size * config.num_heads, \"Embedding dimension must be divisible by number of heads\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-Head Self-Attention [20 Marks]\n",
    "\n",
    "This component is the core of the Transformer. This is where the model learns to attend to different parts of the input sequence, and is the reason why Transformers are so powerful.\n",
    "\n",
    "$$\\text{Attention}(Q, K, V) = \\text{softmax}(\\frac{QK^T}{\\sqrt{d_k}})V$$\n",
    "\n",
    "$$\\text{head}_k = \\text{Attention}(QW^Q_k, KW^K_k, VW^V_k)$$\n",
    "\n",
    "$$\\text{MHSA}(Q, K, V) = \\text{Concat}(\\text{head}_1, ..., \\text{head}_h)W^O$$\n",
    "\n",
    "For simplicity, assume we have a single Head:\n",
    "\n",
    "1. The input has three parts extracted from it: the query $Q$, the key $K$, and the value $V$ (via projections or `Linear` layers). \n",
    "\n",
    "2. The query and key are multiplied together to get a score. This score is then scaled by the square root of the embedding dimension, $\\sqrt{d_k}$, then passed through a softmax to get the attention weights (*after* a masking operation is applied).\n",
    "\n",
    "3. The attention weights are then multiplied with the value to get the final output.\n",
    "\n",
    "When we extend this to *multiple heads*, we simply repeat this process for each head in parallel, and then concatenate the outputs of each head together.\n",
    "\n",
    "Create a class for this `MHSA` module (Multi-Head Self-Attention), adding a comment next to each line in the `forward` method for the shape of the tensor at that point. This will help you debug later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 8, 256])\n",
      "torch.Size([32, 8, 256])\n"
     ]
    }
   ],
   "source": [
    "class MHSA(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.emb_dim = cfg.emb_dim\n",
    "        self.head_size = cfg.head_size\n",
    "        self.block_size = cfg.block_size\n",
    "        self.num_heads = cfg.num_heads\n",
    "\n",
    "        # TODO: Define the single projection layer for QKV\n",
    "        self.projection_qkv = nn.Linear(self.emb_dim, 3 * self.emb_dim)\n",
    "        \n",
    "        # TODO: Define the output projection layer\n",
    "        self.projection_out = nn.Linear(self.emb_dim, self.emb_dim)\n",
    "\n",
    "        # Create a buffer for the mask (buffers are tensors that are not updated during backpropagation)\n",
    "        self.register_buffer(\"mask\", torch.tril(torch.ones(self.block_size, self.block_size)))\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        B, T, C = x.shape # batch size, block size, emb dim\n",
    "        H = self.num_heads\n",
    "\n",
    "        # TODO: Carry out the projections to get the query, key and value - you may want to reshape them to (B, H, T, head_size) to make things easier\n",
    "        qkv = self.projection_qkv(x)\n",
    "        q, k, v = qkv.chunk(3, dim=-1)\n",
    "        q = q.view(B, T, H, -1).permute(0, 2, 1, 3)\n",
    "        k = k.view(B, T, H, -1).permute(0, 2, 1, 3)\n",
    "        v = v.view(B, T, H, -1).permute(0, 2, 1, 3)\n",
    "        \n",
    "        # TODO: Compute the attention scores, perform masking, and apply the softmax\n",
    "        attn_scores = torch.matmul(q, k.transpose(-1, -2)) / torch.sqrt(torch.tensor(self.emb_dim).float())\n",
    "        attn_scores.masked_fill_(self.mask[:T, :T] == 0, float(\"-inf\"))\n",
    "        attn_weights = F.softmax(attn_scores, dim=-1)\n",
    "\n",
    "        # TODO: Compute the weighted sum of the value vectors - you can perform the concatenation via a simple reshaping before applying the final projection\n",
    "        out = torch.matmul(attn_weights, v)\n",
    "        out = out.permute(0, 2, 1, 3).contiguous().view(B, T, -1)\n",
    "        out = self.projection_out(out)\n",
    "        \n",
    "        \n",
    "        return out\n",
    "\n",
    "# !DO NOT REMOVE THESE LINES!\n",
    "x = torch.randn(32, config.block_size, config.emb_dim)\n",
    "print(x.shape)\n",
    "csa = MHSA(config)\n",
    "out = csa(x)\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feedforward Network [10 Marks]\n",
    "\n",
    "The Feedforward Network module usually consists of two linear layers, with a ReLU activation in between. The first linear layer projects the input's Embeddings to a higher dimensionality (by a factor of 4 specifically), and the second linear layer projects it back down to the original dimensionality. This is a very simple module, but is very effective in learning complex functions.\n",
    "\n",
    "Create a class for this `Feedforward` module, with the following tweaks:\n",
    "\n",
    "1. Use the [GELU Activation Function](https://pytorch.org/docs/stable/generated/torch.nn.GELU.html) instead of ReLU between the layers. It is functionally very similar, but has gained a lot of popularity recently.\n",
    "\n",
    "2. Instead of using two `Linear` layers, and an upsampling factor of `4`, use **three** `Linear` layers: the first one upsamples by a factor of `2`, the second one does not change the dimensionality, and the third one downsamples by a factor of `2`. This creates a slightly deeper network, and can be shown to have the same number of parameters as the network in the original Transformer.\n",
    "\n",
    "3. Add a [Dropout layer](https://pytorch.org/docs/stable/generated/torch.nn.Dropout.html) after the last `Linear` layer, with $p=0.1$. This helps mitigate overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Feedforward(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        emb_dim = config.emb_dim\n",
    "\n",
    "        # TODO: Define the feedforward network\n",
    "        self.feedforward = nn.Sequential(\n",
    "            nn.Linear(emb_dim, emb_dim * 2),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(emb_dim * 2, emb_dim),  # Change the input and output dimensions\n",
    "            nn.GELU(),\n",
    "            nn.Linear(emb_dim, emb_dim),  # Change the output dimension\n",
    "            nn.Dropout(0.1)\n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        # TODO: Apply the feedforward network\n",
    "\n",
    "        return self.feedforward(x)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Blocks with Skip Connections [10 Marks]\n",
    "\n",
    "The Encoder consists of multiple *Blocks*, that each contain a Multi-Head Self-Attention module, a Feedforward module, and a Layer Normalization module. We use Skip Connections to help with the flow of gradients during the training process.\n",
    "\n",
    "Create a class for this `Block` module, with the following tweaks:\n",
    "\n",
    "- Instead of creating your own Layer Normalization module, use [the one provided by PyTorch](https://pytorch.org/docs/stable/generated/torch.nn.LayerNorm.html), `nn.LayerNorm`.\n",
    "\n",
    "- Perform the Layer Normalizations **before** the Skip Connections, instead of after. The original paper used the \"post-LN\" approach, but [recent research](https://arxiv.org/abs/2002.04745) has shown that the \"pre-LN\" approach is better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 8, 256])\n",
      "torch.Size([8, 8, 256])\n"
     ]
    }
   ],
   "source": [
    "class Block(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "\n",
    "        # TODO: Define the MHSA, Feedforward, and two LayerNorm modules\n",
    "        self.mhsa = MHSA(config)\n",
    "        self.feedforward = Feedforward(config)\n",
    "        self.layer_norm1 = nn.LayerNorm(config.emb_dim)\n",
    "        self.layer_norm2 = nn.LayerNorm(config.emb_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "\n",
    "        # TODO: Apply the modules with residual connections (use the Pre-LN design)\n",
    "        mhsa_out = self.mhsa(x)\n",
    "        mhsa_out = self.layer_norm1(x + mhsa_out)\n",
    "        feedforward_out = self.feedforward(mhsa_out)\n",
    "        out = self.layer_norm2(mhsa_out + feedforward_out)\n",
    "        \n",
    "        return out\n",
    "\n",
    "\n",
    "# !DO NOT REMOVE THESE LINES!\n",
    "x = torch.randn(8, config.block_size, config.emb_dim)\n",
    "print(x.shape)\n",
    "block = Block(config)\n",
    "out = block(x)\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting it all together [20 Marks]\n",
    "\n",
    "Now that we have all the components, we can put them together to create the Transformer. The Transformer consists of\n",
    "\n",
    "1. Creating Embeddings for the input sequence. This implies turning the input sequence into a sequence of indices, and then passing it through an `nn.Embedding` layer. Alongside this, we will be using another Embedding table for the positional encodings. For simplicity, we can keep the embedding dimensionality for both of these the same.\n",
    "\n",
    "2. Passing the embeddings through a series of Blocks.\n",
    "\n",
    "3. Passing the output of the Encoder through a Layer Normalization layer, and then a Linear layer to get the final logits.\n",
    "\n",
    "Create a class for this `SastaGPT` module. This will be the final model that we will be training. Note that the `generate()` function has been provided for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters: 26.970M\n"
     ]
    }
   ],
   "source": [
    "class SastaGPT(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.emb_dim = config.emb_dim\n",
    "        self.block_size = config.block_size\n",
    "        self.num_layers = config.num_layers\n",
    "        self.vocab_size = config.vocab_size \n",
    "\n",
    "        # TODO: Define the word and position embeddings\n",
    "        # Hint: They will both have the same embedding dimension, only difference is the number of embeddings (vocab_size vs. block_size)\n",
    "        self.word_embeddings = nn.Embedding(config.vocab_size, config.emb_dim)\n",
    "        self.position_embeddings = nn.Embedding(config.block_size, config.emb_dim)\n",
    "        \n",
    "        # TODO: Define the sequence of Blocks\n",
    "        self.blocks = nn.ModuleList([Block(config) for _ in range(config.num_layers)])\n",
    "        \n",
    "        # TODO: Define the final LayerNorm\n",
    "        self.layer_norm = nn.LayerNorm(config.emb_dim)\n",
    "        \n",
    "        # TODO: Define the final linear layer (to get logits)\n",
    "        self.final_linear = nn.Linear(config.emb_dim, config.vocab_size)\n",
    "\n",
    "        # Initialize the weights\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        \n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def forward(self, idxs):\n",
    "        # idxs: (B, T)\n",
    "        batch_size, seq_len = idxs.shape\n",
    "\n",
    "        assert seq_len <= self.block_size, f\"Sequence length exceeds block size of {self.block_size}\"\n",
    "\n",
    "        # TODO: Get the word embeddings (B, T, C) and position embeddings (T, C)\n",
    "        # Hint: For the position embeddings, you can use torch.arange(seq_len) to mimic the indices\n",
    "        # Note: position embeddings are encodings of the position indices (NOT the actual tokens)\n",
    "        position_ids = torch.arange(seq_len, device=idxs.device).unsqueeze(0)\n",
    "        word_embeddings = self.word_embeddings(idxs)\n",
    "        position_embeddings = self.position_embeddings(position_ids)\n",
    "        \n",
    "        # TODO: Add the word and position embeddings (broadcasting will take care of the shapes)\n",
    "        embeddings = word_embeddings + position_embeddings\n",
    "\n",
    "        # TODO: Pass the embeddings through the blocks (B, T, C)\n",
    "        for block in self.blocks:\n",
    "            embeddings = block(embeddings)\n",
    "\n",
    "        # TODO: Apply the final LayerNorm\n",
    "        embeddings = self.layer_norm(embeddings)\n",
    "\n",
    "        # TODO: Apply the final linear layer to get the logits (B, T, V)\n",
    "        logits = self.final_linear(embeddings)\n",
    "\n",
    "        return logits\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def generate(self, idxs, max_new_tokens=20):\n",
    "        '''\n",
    "        Takes in a sequence of indices (the tokenized sentence) and generates new tokens\n",
    "        Note that the input indices should not be longer than the block size\n",
    "        Returns the input sequence with the generated tokens appended (these should be decoded using the Tokenizer)\n",
    "\n",
    "        Params\n",
    "        ------\n",
    "        idxs: torch.Tensor\n",
    "            (B, T) tensor of token indices\n",
    "        max_new_tokens: int\n",
    "            Maximum number of new tokens to generate\n",
    "        '''\n",
    "\n",
    "        # idxs: (B, T)\n",
    "        for _ in range(max_new_tokens):\n",
    "            idxs_trimmed = idxs[:, -self.block_size:] # trim to block size\n",
    "\n",
    "            logits = self(idxs_trimmed) # (B, T, V)\n",
    "\n",
    "            logits = logits[:, -1, :] # (B, V)\n",
    "            \n",
    "            probs = F.softmax(logits, dim=-1) # (B, V)\n",
    "\n",
    "            next_idx = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "\n",
    "            idxs = torch.cat((idxs, next_idx), dim=1) # (B, T+1)\n",
    "            \n",
    "        return idxs\n",
    "\n",
    "cfg = Config()\n",
    "model = SastaGPT(cfg)\n",
    "print(f\"Number of parameters: {sum(p.numel() for p in model.parameters()) / 1e6 :.3f}M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 8])\n",
      "torch.Size([32, 8, 50257])\n"
     ]
    }
   ],
   "source": [
    "# !DO NOT REMOVE THESE LINES!\n",
    "\n",
    "# Check out the forward pass\n",
    "xb, yb = get_batch()\n",
    "print(xb.shape)\n",
    "\n",
    "logits = model(xb)\n",
    "print(logits.shape) # (B, T, V)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now train this model on your own dataset!\n",
    "\n",
    "Note that since the model is quite large, it will take a while to train. This would be a good opportunity to use [Google Colab](https://colab.research.google.com/) or [Kaggle Notebooks](https://www.kaggle.com/notebooks) for free GPU compute. This requires very few changes to your original training code, only having to cast your model and data to the GPU.\n",
    "\n",
    "```python\n",
    "# Define the device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Get the batch\n",
    "xb, yb = get_batch()\n",
    "\n",
    "# Cast to the device\n",
    "xb = xb.to(device)\n",
    "yb = yb.to(device)\n",
    "\n",
    "# Cast the model to the device\n",
    "model = model.to(device)\n",
    "\n",
    "# Forward pass\n",
    "preds = model(xb)\n",
    "\n",
    "...\n",
    "\n",
    "```\n",
    "\n",
    "The only rule is that both your model and your tensors should be on the same device (CPU or GPU)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 0, Loss: 10.87582778930664\n",
      "Step: 100, Loss: 1.5367194414138794\n",
      "Step: 200, Loss: 1.2725788354873657\n",
      "Step: 300, Loss: 1.2424126863479614\n",
      "Step: 400, Loss: 1.4793559312820435\n",
      "Step: 500, Loss: 1.466122031211853\n",
      "Step: 600, Loss: 1.5521754026412964\n"
     ]
    }
   ],
   "source": [
    "# TODO: Train the model\n",
    "\n",
    "# Define the device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Instantiate the SastaGPT model and move it to the device\n",
    "model = SastaGPT(config).to(device)\n",
    "\n",
    "# Define your optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Define the loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Number of training steps\n",
    "num_steps = 610\n",
    "\n",
    "# Training loop\n",
    "for step in range(num_steps):\n",
    "    # Get the batch\n",
    "    xb, yb = get_batch()\n",
    "\n",
    "    # Cast to the device\n",
    "    xb = xb.to(device)\n",
    "    yb = yb.to(device)\n",
    "\n",
    "    # Zero the gradients\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Forward pass\n",
    "    preds = model(xb)\n",
    "\n",
    "    # Compute the loss\n",
    "    loss = criterion(preds.view(-1, config.vocab_size), yb.view(-1))\n",
    "\n",
    "    # Backward pass\n",
    "    loss.backward()\n",
    "\n",
    "    # Update the weights\n",
    "    optimizer.step()\n",
    "\n",
    "    # Print loss every 100 steps\n",
    "    if step % 100 == 0:\n",
    "        print(f\"Step: {step}, Loss: {loss.item()}\")\n",
    "\n",
    "# Save the trained model\n",
    "torch.save(model.state_dict(), 'sastagpt_model.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With your trained model, generate some text. You can use the `generate()` function provided to you.\n",
    "\n",
    "Note that the quality of your generations depends on:\n",
    "\n",
    "- How large the training dataset was. If you used a small dataset, your model will not have enough samples to learn the syntax, semantics, the grammar and other intricacies of language, much less the specifics of the dataset.\n",
    "\n",
    "- How long you trained for. Provided your dataset was large enough, you could try training for longer epochs until the loss values stabilize.\n",
    "\n",
    "- How large your model was. If you used a small model, it will not have enough parameters to learn the complexities of language. You can try increasing the number of layers, the embedding dimensionality, the number of heads, etc.\n",
    "\n",
    "Most importantly, share your generations with the rest of the class! Take some screenshots of your best generations, and share them on the Slack channel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"You're just a big, fat panda!\"\n",
    "idxs = torch.tensor(encoder.encode(sentence)).unsqueeze(0)\n",
    "\n",
    "model.eval()\n",
    "generated = model.generate(idxs, max_new_tokens=100)\n",
    "res = encoder.decode(generated[0].numpy())\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Effects of Hyperparameters on the Number of Parameters [10 Marks]\n",
    "\n",
    "In this section, you will be exploring the effects of the hyperparameters on the number of parameters in your model. \n",
    "\n",
    "With so much room for tweaking (with the embedding dimensionality, the size of the vocab, the number of heads, the number of layers, etc.), it is important to visualize how fast the total number of parameters can grow if you start toying with them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_params(model):\n",
    "    '''\n",
    "    Returns the number of parameters in the model (in millions)\n",
    "    '''\n",
    "    return sum(p.numel() for p in model.parameters()) / 1e6\n",
    "\n",
    "@dataclass\n",
    "class Config:\n",
    "    block_size: int = BLOCK_SIZE\n",
    "    emb_dim: int = 256\n",
    "    head_size: int = 32\n",
    "    num_heads: int = 8\n",
    "    num_layers: int = 2\n",
    "    vocab_size: int = 100 # smaller vocab size for quick prototyping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Plot the number of parameters vs. block size (in multiples of 8, capped at 512)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Plot the number of parameters vs. embedding dimension (in multiples of 256, capped at 1792)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Plot the number of parameters vs. number of heads (in multiples of 4, capped at 32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Plot the number of parameters vs. number of layers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Plot the number of parameters vs. vocabulary size\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After looking at these plots, can you answer which hyperparameters have the most effect on the number of parameters? Which ones have the least effect? Note that even if two plots indicate linearity, it is not necessary their gradients are the same :p\n",
    "\n",
    "<span style=\"color: green\">\n",
    "    Answer here:\n",
    "        \n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fin."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
